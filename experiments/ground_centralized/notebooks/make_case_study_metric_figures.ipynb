{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c51da-4125-44db-a038-df670cf558cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=BIGGER_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=BIGGER_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc(\"lines\", linewidth=4)\n",
    "plt.rc(\"grid\", linestyle=\"--\", color=\"black\", alpha=0.5)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "colors_agents = {\n",
    "    0: \"#f9f06b\",\n",
    "    1: \"#cdaB8f\",\n",
    "    2: \"#dc8add\",\n",
    "    3: \"#99c1f1\",\n",
    "    \"agg-no-sec\": \"#5f0d0d\",\n",
    "    \"agg-sec\": \"#24cd1f\",\n",
    "}\n",
    "\n",
    "colors_trust_metric = {\n",
    "    \"agent\": \"#051b99\",\n",
    "    \"track\": \"#6a7806\",\n",
    "}\n",
    "\n",
    "\n",
    "colors_tracks = list(mcolors.TABLEAU_COLORS.keys())\n",
    "\n",
    "fig_out_dir = os.path.join(\"figures\", \"case_study_metrics\")\n",
    "os.makedirs(fig_out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c8276-5813-406f-8956-c83c474d2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_axes():\n",
    "    # Initialize figure and axes and save to class\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 5), sharex=True, sharey='row')\n",
    "\n",
    "    # left column is for baseline, right for attacked\n",
    "    # top row for OSPA, middle for F1-score, bottom trust metric\n",
    "    loop_tuples = [\n",
    "        (\"{}\", [0, 1], 1),\n",
    "        (\"OSPA Metric\", [0, 10], 0),\n",
    "        # (\"Trust Metric\", [0, 1], 1),\n",
    "    ]\n",
    "    for i_row, (ylabel, ylim, goal) in enumerate(loop_tuples):\n",
    "        for j_col, case in enumerate([\"Benign\", \"Attacked\"]):\n",
    "            ax = axs[i_row, j_col]\n",
    "            title_txt = f\"{case}, {ylabel} (Goal: {goal})\"\n",
    "            ax.set_title(title_txt)\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "            ax.set_ylabel(ylabel)\n",
    "            ax.set_ylim(ylim)\n",
    "            ax.grid()\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "def get_trust_metrics_axes():\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8,2.5), sharey=True)\n",
    "    ylim = [0, 1]\n",
    "    goal = 1\n",
    "    for j_col, case in enumerate([\"Benign\", \"Attacked\"]):\n",
    "        ax = axs[j_col]\n",
    "        title_txt = f\"{case}, (Goal: {goal})\"\n",
    "        ax.set_title(title_txt)\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Trust Metric\")\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.grid()\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff09ceb-665d-4249-831e-74f470df3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avstack_rosbag import DatasetPostprocessor\n",
    "\n",
    "\n",
    "def load_postprocs(case_idx):\n",
    "    case_idx_to_scene = {0: 0, 1: 1, 2: 1}\n",
    "    postproc_benign = DatasetPostprocessor(\n",
    "        bag_folder=\"/data/shared/CARLA/rosbags/baseline_intersection/\",\n",
    "        bag_name=f\"baseline_intersection_{case_idx_to_scene[case_idx]}\",\n",
    "    )\n",
    "    print(f\"Loaded postprocessor of length {len(postproc_benign)}\")\n",
    "    \n",
    "    postproc_attack = DatasetPostprocessor(\n",
    "        bag_folder=\"/data/shared/CARLA/rosbags/cte_case/\",\n",
    "        bag_name=f\"cte_case_{case_idx}_attacked\",\n",
    "    )\n",
    "    print(f\"Loaded postprocessor of length {len(postproc_attack)}\")\n",
    "\n",
    "    return postproc_benign, postproc_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1770265e-fa9f-49c9-85f9-1a9c57d0f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avstack_bridge import MetricsBridge\n",
    "from avstack_bridge import ObjectStateBridge, TrackBridge\n",
    "from avstack.metrics import get_instantaneous_metrics\n",
    "from avtrust_bridge import TrustBridge\n",
    "from avtrust.metrics import get_trust_agents_metrics, get_trust_tracks_metrics\n",
    "\n",
    "\n",
    "def make_metrics_figures(postproc_benign, postproc_attack, case_idx, t_max, assign_metric):\n",
    "    fig, axs = get_metrics_axes()\n",
    "    fig_trust, axs_trust = get_trust_metrics_axes()\n",
    "    \n",
    "    linewidth_small = 2\n",
    "    linewidth_large = 3\n",
    "\n",
    "    metrics_all = {\n",
    "        \"benign\": {\n",
    "            \"no-sec\": {\n",
    "                \"ospa\": None,\n",
    "                \"f1\": None,\n",
    "                \"time\": None,\n",
    "            },\n",
    "            \"sec\": {\n",
    "                \"ospa\": None,\n",
    "                \"f1\": None,\n",
    "                \"agent_trust\": None,\n",
    "                \"track_trust\": None,\n",
    "                \"time\": None,\n",
    "            }\n",
    "        },\n",
    "        \"attacked\": {\n",
    "            \"no-sec\": {\n",
    "                \"ospa\": None,\n",
    "                \"f1\": None,\n",
    "                \"time\": None,\n",
    "            },\n",
    "            \"sec\": {\n",
    "                \"ospa\": None,\n",
    "                \"f1\": None,\n",
    "                \"agent_trust\": None,\n",
    "                \"track_trust\": None,\n",
    "                \"time\": None,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    case_dir = os.path.join(fig_out_dir, f\"case{case_idx}\")\n",
    "    os.makedirs(case_dir, exist_ok=True)\n",
    "\n",
    "    for i_col, postproc in enumerate([postproc_benign, postproc_attack]):\n",
    "        ################################\n",
    "        # PLOT ASSIGNMENT/OSPA METRICS\n",
    "        ################################\n",
    "        # -------------------\n",
    "        # All agents\n",
    "        # -------------------\n",
    "        for ID_agent in range(4):\n",
    "            for sensor in [\"lidar0\"]:\n",
    "                for level in [\"detections\"]:\n",
    "                    # get the metrics data for this agent/sensor/level\n",
    "                    topic = f\"/metrics/agent{ID_agent}/{sensor}/{level}\"\n",
    "                    metrics_msgs = postproc._get_messages_by_time(topic)\n",
    "                    metrics_data = {k: MetricsBridge.assignment_metrics_ros_to_avstack(v) for k, v in metrics_msgs[\"data\"].items()}\n",
    "                    times = np.array(list(metrics_data.keys()))\n",
    "                    ospas = np.array([met.ospa for met in metrics_data.values()])\n",
    "                    f1s = np.array([met.f1_score for met in metrics_data.values()])\n",
    "                    precs = np.array([met.precision for met in metrics_data.values()])\n",
    "                    recalls = np.array([met.recall for met in metrics_data.values()])\n",
    "                    idx_times = times <= t_max\n",
    "    \n",
    "                    # f1 score for agents in top row\n",
    "                    if assign_metric == \"precision\":\n",
    "                        assign_metric_plot = precs\n",
    "                        assign_title = \"Precision\"\n",
    "                    elif assign_metric == \"recall\":\n",
    "                        assign_metric_plot = recalls\n",
    "                        assign_title = \"Recall\"\n",
    "                    elif assign_metric == \"f1-score\":\n",
    "                        assign_metric_plot = f1s\n",
    "                        assign_title = \"F1-Score\"\n",
    "                    else:\n",
    "                        raise NotImplementedError(assign_metric)\n",
    "                    axs[0, i_col].plot(\n",
    "                        times[idx_times],\n",
    "                        assign_metric_plot[idx_times],\n",
    "                        color=colors_agents[ID_agent],\n",
    "                        label=f\"Agent {ID_agent}\",\n",
    "                        linewidth=linewidth_small,\n",
    "                    )\n",
    "                    axs[0, i_col].set_title(axs[0, i_col].get_title().format(assign_title))\n",
    "                    axs[0, i_col].set_ylabel(axs[0, i_col].get_ylabel().format(assign_title))\n",
    "\n",
    "                    # ospa for agents in middle row\n",
    "                    axs[1, i_col].plot(\n",
    "                        times[idx_times],\n",
    "                        ospas[idx_times],\n",
    "                        color=colors_agents[ID_agent],\n",
    "                        label=f\"Agent {ID_agent}\",\n",
    "                        linewidth=linewidth_small,\n",
    "                    )\n",
    "\n",
    "        # -------------------\n",
    "        # Aggregator\n",
    "        # -------------------\n",
    "        loop_tuples = [\n",
    "            (\"Aggregator, No Security\", \"agg-no-sec\", \"/metrics/fusion\"),\n",
    "            (\"Aggregator, Security-Aware\", \"agg-sec\", \"/metrics/security_aware_fusion/assignment\"),\n",
    "        ]\n",
    "        for label, agg_name, topic in loop_tuples:\n",
    "            # load the metrics for the agg\n",
    "            metrics_msgs_agg = postproc._get_messages_by_time(topic)\n",
    "            metrics_data_agg = {k: MetricsBridge.assignment_metrics_ros_to_avstack(v) for k, v in metrics_msgs_agg[\"data\"].items()}\n",
    "            times_agg = np.array(list(metrics_data_agg.keys()))\n",
    "            ospas_agg = np.array([met.ospa for met in metrics_data_agg.values()])\n",
    "            f1s_agg = np.array([met.f1_score for met in metrics_data_agg.values()])\n",
    "            idx_times = times_agg <= t_max\n",
    "\n",
    "            # add the metrics for the aggregator\n",
    "            alg = \"no-sec\" if \"no\" in label.lower() else \"sec\"\n",
    "            case = \"benign\" if i_col == 0 else \"attacked\"\n",
    "            metrics_all[case][alg][\"ospa\"] = ospas_agg[idx_times]\n",
    "            metrics_all[case][alg][\"f1\"] = f1s_agg[idx_times]\n",
    "            metrics_all[case][alg][\"time\"] = times_agg[idx_times]\n",
    "\n",
    "            # add f1 score to plot\n",
    "            axs[0, i_col].plot(\n",
    "                times_agg[idx_times],\n",
    "                f1s_agg[idx_times],\n",
    "                color=colors_agents[agg_name],\n",
    "                linewidth=linewidth_large,\n",
    "                label=label,\n",
    "            )\n",
    "            \n",
    "            # add ospa to plot\n",
    "            axs[1, i_col].plot(\n",
    "                times_agg[idx_times],\n",
    "                ospas_agg[idx_times],\n",
    "                color=colors_agents[agg_name],\n",
    "                linewidth=linewidth_large,\n",
    "                label=label,\n",
    "            )\n",
    "\n",
    "        ################################\n",
    "        # PLOT TRUST METRICS\n",
    "        ################################\n",
    "        trust_loops = [\n",
    "            (\"agent\", TrustBridge.agent_trust_metric_array_ros_to_avstack),\n",
    "            (\"track\", TrustBridge.track_trust_metric_array_ros_to_avstack),\n",
    "        ]\n",
    "        for topic_fill, trust_metric_bridge in trust_loops:\n",
    "            # get the data\n",
    "            topic = f\"/metrics/security_aware_fusion/{topic_fill}_trust\"\n",
    "            metrics_trust_msgs = postproc._get_messages_by_time(topic)\n",
    "            metrics_trust_data = {k: trust_metric_bridge(v) for k, v in metrics_trust_msgs[\"data\"].items()}\n",
    "            times_trust = np.array(list(metrics_trust_data.keys()))\n",
    "            metric_means = np.array([met.mean_metric for met in metrics_trust_data.values()])\n",
    "            idx_times = times_trust <= t_max\n",
    "\n",
    "            # add the metrics for the aggregator\n",
    "            case = \"benign\" if i_col == 0 else \"attacked\"\n",
    "            alg = \"sec\"\n",
    "            metrics_all[case][alg][f\"{topic_fill}_trust\"] = metric_means\n",
    "\n",
    "            # add trust to plot\n",
    "            axs_trust[i_col].plot(\n",
    "                times_trust[idx_times],\n",
    "                metric_means[idx_times],\n",
    "                color=colors_trust_metric[topic_fill],\n",
    "                linewidth=linewidth_large,\n",
    "                label=f\"{topic_fill.title()}\",\n",
    "            )\n",
    "\n",
    "    # add the legend for the assignment/ospas\n",
    "    legitems = [*[\"Agent {}\".format(i) for i in range(4)], \"Aggregator, No Security\", \"Aggregator, Security-Aware\"]\n",
    "    leg = fig.legend(\n",
    "        legitems,\n",
    "        loc=\"lower center\",\n",
    "        bbox_to_anchor=(0.5, -0.15),\n",
    "        ncol=2,\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "        prop={\"family\": \"serif\"},\n",
    "    )\n",
    "    axs_trust[1].legend(\n",
    "        [\"Agent Trust\", \"Track Trust\"],\n",
    "        loc=\"lower right\",\n",
    "        prop={\"family\": \"serif\"},\n",
    "    )\n",
    "\n",
    "    # # show the plot\n",
    "    fig.tight_layout()\n",
    "    fig_trust.tight_layout()\n",
    "\n",
    "    # save metrics figure\n",
    "    fig.savefig(os.path.join(case_dir, f\"metrics_ospa_plot_{case_idx:03d}.pdf\"), bbox_extra_artists=(leg,), bbox_inches='tight')\n",
    "    fig.savefig(os.path.join(case_dir, f\"metrics_ospa_plot_{case_idx:03d}.png\"), bbox_extra_artists=(leg,), bbox_inches='tight')\n",
    "\n",
    "    # save trust metrics figure\n",
    "    fig_trust.savefig(os.path.join(case_dir, f\"metrics_trust_plot_{case_idx:03d}.pdf\"))\n",
    "    fig_trust.savefig(os.path.join(case_dir, f\"metrics_trust_plot_{case_idx:03d}.png\"))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c186f-e351-4b9d-8438-a36b76de08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric_summary_stats(metrics):\n",
    "    # metric reduction compared to baseline\n",
    "    for metric in [\"ospa\"]:\n",
    "        t_eval = metrics[\"benign\"][\"no-sec\"][\"time\"]\n",
    "        sec_att_interp = np.interp(t_eval, metrics[\"attacked\"][\"sec\"][\"time\"], metrics[\"attacked\"][\"sec\"][metric])\n",
    "        nosec_att_interp = np.interp(t_eval, metrics[\"attacked\"][\"no-sec\"][\"time\"], metrics[\"attacked\"][\"no-sec\"][metric])\n",
    "        benign_result = metrics[\"benign\"][\"no-sec\"][metric]\n",
    "        nosec_error_vs_benign = np.maximum(0, nosec_att_interp - benign_result)\n",
    "        sec_error_vs_benign = np.maximum(0, sec_att_interp - benign_result)\n",
    "        sec_error_reduction = (nosec_error_vs_benign - sec_error_vs_benign) / nosec_error_vs_benign\n",
    "        sec_error_reduction = sec_error_reduction[~np.isinf(sec_error_reduction)]\n",
    "        print(f\"Mean error reduction for {metric}: {np.nanmean(sec_error_reduction):.4f}\")\n",
    "\n",
    "    # average trust accuracy\n",
    "    for case in metrics:\n",
    "        for target in [\"agent_trust\", \"track_trust\"]:\n",
    "            print(f\"Average {target} metric for {case}: {np.mean(metrics[case]['sec'][target]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0efec-af4e-4727-b00b-35634a79fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = {0: \"precision\", 1: \"precision\", 2: \"recall\"}\n",
    "for i_case in [1]: #range(3):\n",
    "    postproc_benign, postproc_attack = load_postprocs(i_case)\n",
    "    metrics_all = make_metrics_figures(postproc_benign, postproc_attack, i_case, t_max=10, assign_metric=metric_to_plot[i_case])\n",
    "    print_metric_summary_stats(metrics_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beecb78e-454a-4097-8c85-e87eef4a28ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
